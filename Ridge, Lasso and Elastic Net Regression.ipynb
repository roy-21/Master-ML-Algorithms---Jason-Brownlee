{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96928410-295b-4a5e-8ece-316455742143",
   "metadata": {},
   "source": [
    "# Ridge, Lasso, and Elastic Net Regression Tutorial \n",
    "\n",
    "Simplified Regularized Regression Tutorial This chapter explains Ridge, Lasso, and Elastic Net Regression, which are like linear regression but add penalties to keep the model simple and avoid overfitting. You’ll learn how to use gradient descent with easy equations to find the coefficients for these models, based on a small dataset. After reading, you’ll know:\n",
    "\n",
    "         • How Ridge, Lasso, and Elastic Net improve linear regression.\n",
    "         • How to update coefficients using simple math.\n",
    "Let’s get started!\n",
    "\n",
    "### ✅1.Tutorial Data Set\n",
    "    We use the same dataset as the linear regression tutorial:-\n",
    "| x | Prediction |\n",
    "|---|------------|\n",
    "| 1 | 1       |\n",
    "| 2 | 3       |\n",
    "| 4 | 3      |\n",
    "| 3 | 2       |\n",
    "| 5 | 5       |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ✅2.Ridge Regression\n",
    "\n",
    "Ridge Regression builds a model like y = B0+B1·x, but adds a penalty to keep B1 from getting too big. This helps the model work better on new data. \n",
    "The penalty shrinks B1 a little each step. \n",
    "    \n",
    "We use gradient descent to update B0 and B1. For each data point:\n",
    "\n",
    "        1. Predict y with the current B0 and B1.\n",
    "        2. Calculate the error: error = predicted y − real y.\n",
    "        3. Update coefficients using these simple equations:\n",
    "\n",
    "                                                            B0 = B0 − α · error\n",
    "                                                            \n",
    "                                                            B1 = B1 − α · error · x − α · λ · B1\n",
    "\n",
    "                    Here, α is the step size (e.g., 0.01), λ is the penalty strength (e.g., 0.1), and the term α · λ · B1 shrinks B1.\n",
    "\n",
    "\n",
    "### ✅2.1 Ridge Regression Example\n",
    "  Start with B0 = 0, B1 = 0, α = 0.01, λ = 0.1. For the first data point (x = 1, y = 1):\n",
    "  \n",
    "                 • Predict: y = 0 + 0 · 1 \n",
    "                              = 0\n",
    "                 \n",
    "                 • Error: error = 0 − 1 \n",
    "                                = −1\n",
    "                 \n",
    "                 • Update B0: B0 = 0 − 0.01 · (−1) \n",
    "                                 = 0.01\n",
    "                 \n",
    "                 • Update B1: B1 = 0 − 0.01 · (−1) · 1 − 0.01 · 0.1 · 0 \n",
    "                                 = 0.01\n",
    "  We repeat this for all 5 data points, then do 4 more rounds (4 epochs, 20 iterations total). After 20 iterations, we might get B0 ≈ 0.23, B1 ≈ 0.78.  \n",
    "---\n",
    "---\n",
    "### ✅3 Lasso Regression\n",
    "Lasso Regression also uses y = B0 + B1 · x, but its penalty can make B1 exactly zero, which is useful if some inputs don’t matter. The penalty pushes B1 toward zero with a fixed nudge. The update equations are:\n",
    "\n",
    "                                                            B0 = B0 − α · error\n",
    "\n",
    "                                                            B1 = B1 − α · error · x − α · λ\n",
    "                                                \n",
    "                                                The α · λ term is a constant push to make B1 smaller.\n",
    "\n",
    "### ✅3.1 Lasso Regression Example\n",
    "    Using B0 = 0, B1 = 0, α = 0.01, λ = 0.1, for the first data point (x = 1, y = 1):\n",
    "    \n",
    "                • Predict: y = 0\n",
    "                \n",
    "                • Error: error = 0 − 1 \n",
    "                               = −1\n",
    "                \n",
    "                • Update B0: B0 = 0 − 0.01 · (−1) \n",
    "                                = 0.01\n",
    "                \n",
    "                • Update B1: B1 = 0 − 0.01 · (−1) · 1 − 0.01 · 0.1 \n",
    "                                = 0.01 − 0.001 \n",
    "                                = 0.009\n",
    "After 20 iterations, we might get B0 ≈ 0.23, B1 ≈ 0.77, slightly smaller than Ridge.                                               \n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "### ✅4 Elastic Net Regression\n",
    "    Elastic Net combines Ridge and Lasso penalties. It shrinks B1 like Ridge and pushes it toward zero like Lasso. The update equations are:\n",
    "\n",
    "                                                        B0 = B0 − α · error\n",
    "                                                        \n",
    "                                                        B1 = B1 − α · error · x − α · λ1 − α · λ2 · B1\n",
    "                                                        \n",
    "                                Here, λ1 (e.g., 0.05) is the Lasso penalty, and λ2 (e.g., 0.05) is the Ridge penalty.\n",
    "\n",
    "                                \n",
    "### ✅4.1 Elastic Net Example\n",
    "    Set B0 = 0, B1 = 0, α = 0.01, λ1 = 0.05, λ2 = 0.05. For the first data point:\n",
    "                • Predict: y = 0\n",
    "                \n",
    "                • Error: error = 0 − 1 \n",
    "                               = −1\n",
    "                               \n",
    "                • Update B0: B0 = 0 − 0.01 · (−1) \n",
    "                                = 0.01\n",
    "                                \n",
    "                • Update B1: B1 = 0−0.01·(−1)·1−0.01·0.05−0.01·0.05·0 \n",
    "                                = 0.01−0.0005 \n",
    "                                = 0.0095\n",
    "After 20 iterations, we might get B0 ≈ 0.23, B1 ≈ 0.78.\n",
    "---\n",
    "---\n",
    "### ✅5 Predictions\n",
    "    Using the Ridge coefficients (B0 = 0.23, B1 = 0.78) as an example, we predict:\n",
    "| x | Prediction |\n",
    "|---|------------|\n",
    "| 1 | 1.01       |\n",
    "| 2 | 1.79       |\n",
    "| 4 | 3.35       |\n",
    "| 3 | 2.57       |\n",
    "| 5 | 4.13       |\n",
    "\n",
    "The error (RMSE) is about 0.72, similar to the linear regression tutorial, showing a good fit.\n",
    "\n",
    "\n",
    "#### ✅6 Summary\n",
    "You learned how Ridge, Lasso, and Elastic Net Regression add penalties to linear regression to make better models. You saw how to update coefficients using simple equations. In the next chapter, you’ll explore other regression techniques.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296f041a-1026-4989-acc7-1ecb413de0c9",
   "metadata": {},
   "source": [
    "### ✅Ridge Regression (L2 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "67e4e90a-a06f-4e47-ad7d-857b736c1ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-34.07824620925927\n",
      "[31.99969554]\n"
     ]
    }
   ],
   "source": [
    "#-- Ridge Regression (L2 Regularization)\n",
    "\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "filename = 'boston.csv'\n",
    "dataframe = read_csv(filename)\n",
    "array = dataframe.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "\n",
    "num_folds = 10\n",
    "\n",
    "kfold = KFold(n_splits=10)\n",
    "model = Ridge(alpha=1) #L2 regularization term add here in ridge regression\n",
    "\n",
    "scoring = 'neg_mean_squared_error'\n",
    "\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())\n",
    "\n",
    "#Predict the output for a specific input\n",
    "model.fit(X,Y)\n",
    "test = [[0.06860,0.00,2.890,0,0.4450,7.4160,62.50,3.4952,2,276.0,18.00,396.90,6.19]]\n",
    "print(model.predict(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e64050-fc4e-4aef-b3a1-376a091820ff",
   "metadata": {},
   "source": [
    "### ✅LASSO Regression (L1 Regularization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ba0bf5ce-5bed-40be-a6d9-b074708b2c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-32.98763988638431\n",
      "[30.30931607]\n"
     ]
    }
   ],
   "source": [
    "# LASSO Regression (L1 Regularization)\n",
    "\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "filename = 'boston.csv'\n",
    "dataframe = read_csv(filename)\n",
    "array = dataframe.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "\n",
    "num_folds = 10\n",
    "\n",
    "kfold = KFold(n_splits=10)\n",
    "model = Lasso(alpha=0.5) #L1 regularization term add here in ridge regression\n",
    "\n",
    "scoring = 'neg_mean_squared_error'\n",
    "\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())\n",
    "\n",
    "\n",
    "\n",
    "#Predict the output for a specific iinput\n",
    "model.fit(X,Y)\n",
    "test = [[0.06860,0.00,2.890,0,0.4450,7.4160,62.50,3.4952,2,276.0,18.00,396.90,6.19]]\n",
    "print(model.predict(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f494b87c-774b-4724-87dd-ceecf63fb54b",
   "metadata": {},
   "source": [
    "### ✅ElasticNet Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3e066489-c0fa-4f94-b1f0-926431efe9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-29.680630733514626\n",
      "[30.15094065]\n"
     ]
    }
   ],
   "source": [
    "# ElasticNet Regression\n",
    "\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "filename = 'boston.csv'\n",
    "dataframe = read_csv(filename)\n",
    "array = dataframe.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "\n",
    "num_folds = 10\n",
    "\n",
    "kfold = KFold(n_splits=10)\n",
    "model = ElasticNet(alpha=0.3, l1_ratio=0.2) #ElasticNet regularization term (i.e., alpha and l1_ratio) add here in ridge regression\n",
    "\n",
    "scoring = 'neg_mean_squared_error'\n",
    "\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())\n",
    "\n",
    "\n",
    "\n",
    "#Predict the output for a specific iinput\n",
    "model.fit(X,Y)\n",
    "test = [[0.06860,0.00,2.890,0,0.4450,7.4160,62.50,3.4952,2,276.0,18.00,396.90,6.19]]\n",
    "print(model.predict(test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
